<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>辞</title>
  
  <subtitle>迷雾里寻不见人, 那就把自己化作灯塔.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-04-09T01:56:01.145Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Ci</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>梯度累积</title>
    <link href="http://yoursite.com/2021/04/09/20210409-%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF/"/>
    <id>http://yoursite.com/2021/04/09/20210409-%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF/</id>
    <published>2021-04-08T16:12:00.000Z</published>
    <updated>2021-04-09T01:56:01.145Z</updated>
    
    <content type="html"><![CDATA[<h2 id="梯度累积"><a href="#梯度累积" class="headerlink" title="梯度累积"></a>梯度累积</h2><p>Question：GPU显存不足的情况下，如何增加batch_size?</p><p>Ans：将batch数据，分成多个mini_batch, 然后进行<code>梯度累积</code>；</p><h3 id="原代码"><a href="#原代码" class="headerlink" title="原代码"></a>原代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">predictions = model(inputs)               <span class="comment"># Forward pass</span></span><br><span class="line">loss = loss_function(predictions, labels) <span class="comment"># Compute loss function</span></span><br><span class="line">loss.backward()                           <span class="comment"># Backward pass</span></span><br><span class="line">optimizer.step()                          <span class="comment"># Optimizer step</span></span><br><span class="line">predictions = model(inputs)               <span class="comment"># Forward pass with new parameters</span></span><br></pre></td></tr></table></figure><p>在 loss.backward() 运算期间，为每个参数计算梯度，并将其存储在与每个参数相关联的张量——parameter.grad 中.</p><hr><h3 id="梯度累积代码"><a href="#梯度累积代码" class="headerlink" title="梯度累积代码"></a>梯度累积代码</h3><p>累积梯度意味着，在调用 optimizer.step() 实施一步梯度下降之前，我们会对 parameter.grad 张量中的几个反向运算的梯度求和。在 PyTorch 中这一点很容易实现，因为梯度张量在不调用 model.zero_grad() 或 optimizer.zero_grad() 的情况下不会重置。<strong>如果损失在训练样本上要取平均，我们还需要除以累积步骤的数量</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model.zero_grad()                                   <span class="comment"># Reset gradients tensors</span></span><br><span class="line"><span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> enumerate(training_set):</span><br><span class="line">    predictions = model(inputs)                     <span class="comment"># Forward pass</span></span><br><span class="line">    loss = loss_function(predictions, labels)       <span class="comment"># Compute loss function</span></span><br><span class="line">    loss = loss / accumulation_steps                <span class="comment"># Normalize our loss (if averaged)</span></span><br><span class="line">    loss.backward()                                 <span class="comment"># Backward pass</span></span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % accumulation_steps == <span class="number">0</span>:             <span class="comment"># Wait for several backward steps</span></span><br><span class="line">        optimizer.step()                            <span class="comment"># Now we can do an optimizer step</span></span><br><span class="line">        model.zero_grad()                           <span class="comment"># Reset gradients tensors</span></span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>) % evaluation_steps == <span class="number">0</span>:           <span class="comment"># Evaluate the model when we...</span></span><br><span class="line">            evaluate_model()                        <span class="comment"># ...have no gradients accumulated</span></span><br></pre></td></tr></table></figure><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gpd9bg357xj30q408m3zj.jpg" style="zoom: 50%;" /><hr><p>引：<a href="https://www.jiqizhixin.com/articles/2018-10-17-11" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2018-10-17-11</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;梯度累积&quot;&gt;&lt;a href=&quot;#梯度累积&quot; class=&quot;headerlink&quot; title=&quot;梯度累积&quot;&gt;&lt;/a&gt;梯度累积&lt;/h2&gt;&lt;p&gt;Question：GPU显存不足的情况下，如何增加batch_size?&lt;/p&gt;
&lt;p&gt;Ans：将batch数据，分成多个m
      
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="gradient_accumulate" scheme="http://yoursite.com/tags/gradient-accumulate/"/>
    
  </entry>
  
  <entry>
    <title>梯度裁剪(gradient_clip_norm)</title>
    <link href="http://yoursite.com/2021/04/08/20210408-%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA(gradient_clip_norm)/"/>
    <id>http://yoursite.com/2021/04/08/20210408-%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA(gradient_clip_norm)/</id>
    <published>2021-04-08T14:00:00.000Z</published>
    <updated>2021-04-08T14:00:57.839Z</updated>
    
    <content type="html"><![CDATA[<h2 id="梯度截断-Gradient-Clip-Norm"><a href="#梯度截断-Gradient-Clip-Norm" class="headerlink" title="梯度截断(Gradient Clip Norm)"></a>梯度截断(Gradient Clip Norm)</h2><p>引：<a href="https://blog.csdn.net/csnc007/article/details/97804398" target="_blank" rel="noopener">https://blog.csdn.net/csnc007/article/details/97804398</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils <span class="keyword">import</span> clip_grad_norm</span><br></pre></td></tr></table></figure><p>用来解决梯度爆炸问题：</p><ul><li>首先设置一个梯度阈值：clip_gradient</li><li>在后向传播中求出各参数的梯度，这里不直接使用梯度进行参数更新，而是求这些梯度的l2范数</li><li>然后比较梯度的l2范数||g||与clip_gradient的大小</li><li>如果前者大，求缩放因子clip_gradient/||g|| (梯度越大缩放因子就越小)</li><li>最后将梯度乘上缩放因子得到最后所需要的梯度</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">loss = model(...)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;梯度截断-Gradient-Clip-Norm&quot;&gt;&lt;a href=&quot;#梯度截断-Gradient-Clip-Norm&quot; class=&quot;headerlink&quot; title=&quot;梯度截断(Gradient Clip Norm)&quot;&gt;&lt;/a&gt;梯度截断(Gradient Cl
      
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="clip_grad_norm" scheme="http://yoursite.com/tags/clip-grad-norm/"/>
    
  </entry>
  
  <entry>
    <title>LSA&amp;LDA</title>
    <link href="http://yoursite.com/2021/02/24/2021224-LSA&amp;LDA/"/>
    <id>http://yoursite.com/2021/02/24/2021224-LSA&amp;LDA/</id>
    <published>2021-02-24T02:21:00.000Z</published>
    <updated>2021-02-24T02:48:06.759Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://www.jianshu.com/p/526084cb735d" target="_blank" rel="noopener">https://www.jianshu.com/p/526084cb735d</a></p><h2 id="LSA-LDA-关键词提取"><a href="#LSA-LDA-关键词提取" class="headerlink" title="LSA/LDA - 关键词提取"></a>LSA/LDA - 关键词提取</h2><p>主题模型认为<strong>词</strong>与<strong>文档</strong>之间没有直接的联系，它们应当有一个维度(<strong>主题</strong>)将它们联系起来。</p><p><strong>每个文档对应着一个或多个主题，每个主题都有对应的词分布，通过主题，就可以得到每个文档的词分布</strong>。</p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnyewi16o1j30fx0g774s.jpg" style="zoom:50%;" /><h3 id="LSA-潜在语义分析模型"><a href="#LSA-潜在语义分析模型" class="headerlink" title="LSA-潜在语义分析模型"></a>LSA-潜在语义分析模型</h3><p>在潜在语义分析(LSA)模型首先给出了这样一个“分布式假设”:一个单词的属性是由它所处的环境刻画的。这也就意味着如果两个单词在含义上比较接近，那么它们也会出现在相似的文本中，也就是说具有相似的上下文。</p><p>LSA首先构建了这样一个 ‘‘单词-文档’’ 矩阵:矩阵的每一行表示一个单词，矩阵的每一列表示一个文章，第<em>i</em>行第<em>j</em>列的值表示第<em>i</em>个单词在第<em>j</em>个段落里面出现了几次或者表示该单词的tf-idf值等等。LSA模型在构建好了单词-文档矩阵之后，出于以下几种可能的原因，我们会使用奇异值分解(Singular Value Decomposition，SVD)的方法来寻找该矩阵的一个低阶近似:</p><ul><li>原始的单词-文档矩阵过于庞大而会消耗过多的计算资源。在这种情况下，近似的低阶矩阵可以被解释为原始矩阵的一个‘‘近似’’。</li><li>原始的单词-文档矩阵往往包含着很多噪音，也就是说并不是里面的任何一个信息都是有用的。在这种情况下，求近似的矩阵的过程可以看成对原来的矩阵进行‘‘降噪’’。</li><li>原始的单词-文档矩阵相对于‘‘真实的’’单词-文档矩阵而言过于稀疏。所谓真实的矩阵，就是指将世界上所有出现的单词和文档都考虑在内得到的矩阵，而这显然是不可能的。我们只能通过分析一部分数据来得到真实矩阵的一种近似。在这种情况下，近似的矩阵可以看成是原始矩阵的一种‘‘精简’’版本。</li></ul><h3 id="PLSA-概率潜在语义分析模型"><a href="#PLSA-概率潜在语义分析模型" class="headerlink" title="PLSA-概率潜在语义分析模型"></a>PLSA-概率潜在语义分析模型</h3><p>概率潜在语义分析(PLSA)模型其实是为了克服潜在语义分析(LSA)模型存在的一些缺点而被提出的。LSA的一个根本问题在于，尽管我们可以把<em>U</em>k和<em>V</em>k的每一列都看成是一个话题，但是由于每一列的值都可以看成是几乎没有限制的实数值，因此我们无法去进一步解释这些值到底是什么意思，也更无法从概率的角度来理解这个模型。而寻求概率意义上的解释则是贝叶斯推断的核心思想之一。</p><p><strong>PLSA模型则通过一个生成模型来为LSA赋予了概率意义上的解释。该模型假设，每一篇文档都包含一系列可能的潜在话题，文档中的每一个单词都不是凭空产生的，而是在这些潜在的话题的指引下通过一定的概率生成的。</strong></p><p>在 PLSA 模型里面，话题其实是一种单词上的概率分布，每一个话题都代表着一个不同的单词上的概率分布，而每个文档又可以看成是话题上的概率分布。每篇文档就是通过这样一个两层的概率分布生成的，这也正是PLSA 提出的生成模型的核心思想。</p><p>PLSA 通过下面这个式子对 <em>d</em> 和 w 的联合分布进行了建模:</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnyf9jd07zj30u004jaa7.jpg" alt="640"></p><p>该模型中的<em>z</em>的数量是需要事先给定的一个超参数。需要注意的是，上面这个式子里面给出了 <em>P</em>(w, <em>d</em>)的两种表达方式，在前一个式子里，<em>d</em>和w都是在给定<em>z</em>的前提下通过条件概率生成出来的，它们的生成方式是相似的，因此是‘‘对称’’的; 在后一个式子里，首先给定<em>d</em>，然后根据<em>P</em>(<em>z</em>|<em>d</em>)生成可能的话题 <em>z</em>，然后再根据 <em>P</em>(w|<em>z</em>) 生成可能的单词w，由于在这个式子里面单词和文档的生成并不相似，所以是‘‘非对称’’的。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnyfco7zq8j30u00bkdgi.jpg" alt="640"></p><p>PLSA 模型中非对称形式的 Plate Notation表示法。其中<em>d</em>表示一篇文档，<em>z</em>表示由文档生成的一个话题，w表示由话题生成的一个单词。<strong>在这个模型中，<em>d</em>和w是已经观测到的变量，而<em>z</em>是未知的变量(代表潜在的话题)。</strong> </p><p><font color=red>容易发现，对于一个新的文档而言，我们无法得知它对应的 <em>P</em>(<em>d</em>) 究竟是什么， 因此尽管PLSA 模型在给定的文档上是一个生成模型，它却无法生成新的未知的文档。该模型的另外的一个问题在于，随着文档数量的增加，<em>P</em>(<em>z</em>|<em>d</em>)的参数也会随着线性增加，这就导致无论有多少训练数据，都容易导致模型的过拟合问题。这两点成为了限制 PLSA 模型被更加广泛使用的两大缺陷。</font></p><h3 id="LDA-潜在狄利克雷分配模型"><a href="#LDA-潜在狄利克雷分配模型" class="headerlink" title="LDA-潜在狄利克雷分配模型"></a>LDA-潜在狄利克雷分配模型</h3><p>从根本上来讲，LDA 模型是在 PLSA 的模型的基础上引入了参数的先验分布这个概念。</p><p>在 PLSA 这个模型里，对于一个未知的新文档<em>d</em>，我们对于<em>P</em>(<em>d</em>)一无所知，而这个其实是不符合人的经验的。或者说，它没有去使用本来可以用到的信息，而这部分信息就是LDA中所谓的<strong>先验信息</strong>。</p><p>具体来说，在LDA中，首先每一个文档都被看成跟有限个给定话题中的每一个存在着或多或少的关联性，而这种关联性则是用话题上的概率分布来刻画的，这一点与PLSA其实是一致的。</p><p>但是在 LDA 模型中，每个文档关于话题的概率分布都被赋予了一个先验分布，这个先验一般是用稀疏形式的狄利克雷分布表示的。 这种稀疏形式的狄利克雷先验可以看成是编码了人类的这样一种先验知识:一般而言，一篇文章的主题更有可能是集中于少数几个话题上，而很少说在单独一篇文章内同时在很多话题上都有所涉猎并且没有明显的重点。</p><p>此外，LDA 模型还对一个话题在所有单词上的概率分布也赋予了一个稀疏形式的狄利克雷先验，它的直观解释也是类似的:在一个单独的话题中，多数情况是少部分(跟这个话题高度相关的)词出现的频率会很高，而其他的词出现的频率则明显较低。这样两种先验使得 LDA 模型能够比 PLSA 更好地刻画文档-话题-单词这三者的关系。</p><p><font color=green>事实上，从PLSA的结果上来看，它实际上相当于把LDA模型中的先验分布转变为均匀分布，然后对所要求的参数求最大后验估计(在先验是均匀分布的前提下，这也等价于求参数的最大似然估计)，而这也正反映出了一个较为合理的先验对于建模是非常重要的。</font></p><img src="https://image.jiqizhixin.com/uploads/editor/c9d68859-db6b-469e-a777-b2cbb3965ffb/640.png" alt="640" style="zoom:50%;" />]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://www.jianshu.com/p/526084cb735d&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/526084cb735d&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=
      
    
    </summary>
    
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="nlp" scheme="http://yoursite.com/tags/nlp/"/>
    
      <category term="LSA" scheme="http://yoursite.com/tags/LSA/"/>
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
  </entry>
  
  <entry>
    <title>Attention机制</title>
    <link href="http://yoursite.com/2021/02/18/20210218-Attention%E6%9C%BA%E5%88%B6/"/>
    <id>http://yoursite.com/2021/02/18/20210218-Attention%E6%9C%BA%E5%88%B6/</id>
    <published>2021-02-18T02:19:00.000Z</published>
    <updated>2021-02-20T03:31:07.797Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://zhuanlan.zhihu.com/p/53682800" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/53682800</a></p><h2 id="Attention机制思想"><a href="#Attention机制思想" class="headerlink" title="Attention机制思想"></a>Attention机制思想</h2><p>当用神经网络来处理大量的输入信息时，可以借鉴人脑🧠的注意力机制，只选择一些<strong>关键</strong>的信息输入来进行处理，从而提高神经网络的效率。</p><h2 id="Attention计算流程"><a href="#Attention计算流程" class="headerlink" title="Attention计算流程"></a>Attention计算流程</h2><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnrh59pxzjj30hs07mwez.jpg" alt="v2-54fe529ded98721f35277a5bfa79febc_r" style="zoom: 67%;" /><p><strong>Attention机制的实质就是个寻址过程</strong>。给定一个和任务相关的查询<strong>Query</strong>向量q，通过计算与<strong>Key</strong>的注意力分布并附加在<strong>Value</strong>上，从而计算出<strong>Attention Value</strong>。</p><p>不需要将所有的N个输入信息都输入到神经网络进行计算，只需要从X中选择一些<strong>和任务相关</strong>的信息输入给神经网络。</p><h3 id="三步"><a href="#三步" class="headerlink" title="三步"></a>三步</h3><ol><li><h4 id="信息输入"><a href="#信息输入" class="headerlink" title="信息输入"></a>信息输入</h4><p><strong>X</strong> = [x1,…,xN]表示N个输入信息；</p></li><li><h4 id="计算注意力分布-alpha"><a href="#计算注意力分布-alpha" class="headerlink" title="计算注意力分布$\alpha$"></a>计算注意力分布$\alpha$</h4><p>令key=value=<strong>X</strong>, 给出注意力分布：</p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnrheboo6hj30lm01gq2y.jpg" style="zoom: 33%;" /><p>将$\alpha_i$称为注意力分布(概率分布)，s(Xi,q)为注意力打分机制。打分机制有多种：</p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnrhh9z4wwj30ow07swf3.jpg" style="zoom: 33%;" /></li><li><h4 id="根据注意力分布-alpha-计算输入信息的加权平均"><a href="#根据注意力分布-alpha-计算输入信息的加权平均" class="headerlink" title="根据注意力分布$\alpha$计算输入信息的加权平均"></a>根据注意力分布$\alpha$计算输入信息的加权平均</h4><p>注意力分布$\alpha_i$可以理解为在上下文查询q时，第i个信息受关注的程度。</p><p>采用一种“软性”的信息选择机制对输入信息X进行编码为：</p><p>$att(q,X) = \sum_{i=1}^N \alpha_i X_i$ (软性注意力机制)</p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnttwb4wwzj30gc08w750.jpg" alt="v2-aa371755dc73b7137149b8d2905fc4ba_r" style="zoom:50%;" /><p>软性注意力机制有两种：普通模式(key=value=X)和键值对模式(key!=value).</p></li></ol><h2 id="Attention机制的变种"><a href="#Attention机制的变种" class="headerlink" title="Attention机制的变种"></a>Attention机制的变种</h2><h3 id="变种-多头注意力-Multi-head-Attention"><a href="#变种-多头注意力-Multi-head-Attention" class="headerlink" title="变种-多头注意力(Multi-head Attention)"></a>变种-多头注意力(Multi-head Attention)</h3><p>多头注意力(Multi-head Attention)是利用多个查询Q=[q_1,…,q_M]，来平行地计算从从输入信息中选取多个信息。每个注意力关注输入信息的不同部分，然后进行拼接：</p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gntubcdjwsj30d4016gli.jpg" alt="v2-27673fff36241d6ef163c9ac1cedcce7_r" style="zoom:67%;" />]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53682800&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/53682800&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;A
      
    
    </summary>
    
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="nlp" scheme="http://yoursite.com/tags/nlp/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>Aspect-level情感分析</title>
    <link href="http://yoursite.com/2021/02/14/20210214-Aspect-level%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2021/02/14/20210214-Aspect-level%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/</id>
    <published>2021-02-14T10:25:00.000Z</published>
    <updated>2021-02-14T10:39:16.010Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://zhuanlan.zhihu.com/p/79387620" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79387620</a></p><h2 id="Aspect-level情感分析"><a href="#Aspect-level情感分析" class="headerlink" title="Aspect-level情感分析"></a>Aspect-level情感分析</h2><ul><li>文档级：假设整个文档只针对一个话题表达了情感（实际通常不是这样）；</li><li>句子级：假设一个句子只针对一个话题表达了情感；</li><li>Aspect级：在文档级和句子级情感分析中，计算出的情感值是整个文档或整句话的情感值，并未与某话题直接关联；而在<strong>Aspect-level情感分析</strong>中，提取的是“对象+情感值”，表达情感的<strong>对象</strong>往往是<strong>分析目标的属性或特征</strong>，因此挖掘出了更多更细的信息，例如在餐厅评论的Aspect-level情感分析中，“对象+情感值”可以是“价格，-1”、“味道，2”、“新鲜，1”；</li></ul><br/><p>Aspect-level情感分析的主要步骤：</p><ol><li><p>从文本中识别“情感值+对象”；</p></li><li><p>将“情感值+对象”分类；</p></li><li><ul><li>可以根据预设的情感阈值（e.g.正负面）</li><li>也可以根据预设的aspects</li></ul></li><li><p>聚合各个aspect的情感值；</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/79387620&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/79387620&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;A
      
    
    </summary>
    
    
      <category term="sentiment analysis" scheme="http://yoursite.com/categories/sentiment-analysis/"/>
    
    
      <category term="nlp" scheme="http://yoursite.com/tags/nlp/"/>
    
      <category term="sentiment analysis" scheme="http://yoursite.com/tags/sentiment-analysis/"/>
    
      <category term="Aspect-level" scheme="http://yoursite.com/tags/Aspect-level/"/>
    
  </entry>
  
  <entry>
    <title>神经网络知识 - 卷积层、池化层、全连接层</title>
    <link href="http://yoursite.com/2021/02/05/20210205-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86-%E5%8D%B7%E7%A7%AF%E5%B1%82%E3%80%81%E6%B1%A0%E5%8C%96%E5%B1%82%E3%80%81%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82/"/>
    <id>http://yoursite.com/2021/02/05/20210205-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86-%E5%8D%B7%E7%A7%AF%E5%B1%82%E3%80%81%E6%B1%A0%E5%8C%96%E5%B1%82%E3%80%81%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82/</id>
    <published>2021-02-05T04:50:00.000Z</published>
    <updated>2021-02-05T06:10:04.709Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://www.jianshu.com/p/ac772ec40192" target="_blank" rel="noopener">https://www.jianshu.com/p/ac772ec40192</a></p><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p><strong>作用：对输入的图片进行特征提取</strong></p><p>过程：输入的图片与kernel进行卷积得出输出。</p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnckdxx1qlj312c0j0ace.jpg" style="zoom:67%;" /><p>激活函数：卷积层的最后一部分。作用：增加输出的非线性。常用ReLu或者Tanh激活函数。</p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnckfbmrkdj312c0j0ace.jpg" style="zoom:67%;" /><br/><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p><strong>作用：降低输入图片的尺寸</strong></p><p>卷积神经网络中，每个卷积层后面总会跟一个池化层。池化层的作用是加速运算并且使得一些检测到的特征更robust。</p><p>也会用到kernel和步长。下图中2*2的filter用于池化4*4的输入，步长为2.</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnckijn7c1j311q0c2q41.jpg" alt=""></p><p>左边为最大池化、右边为平均池化。</p><br/><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>全连接层出现在卷积神经网络的末尾。</p><p>全连接层之前，前面很多层所产生的<code>feature map</code>被压缩成一个向量。这个向量被喂到全连接层中。</p><p>全连接层的输出是一个<strong>一维</strong>的特征向量。</p><p>E.g 图像二分类.</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gncltgvx08j311g0bmq79.jpg" alt=""></p><p>输入为32*32*3的RGB三通道图，输出尺寸为2(二分类)。第一层为5*5*3kernel的卷积层，第二层为kernel尺寸为2的最大池化层，第三层为5*5*3kernel的卷积层，第四层为kernel尺寸为2的最大池化层，其输出被压缩为一个向量。被喂到最后两层全连接层。</p><p>注⚠️：对于RGB三通道的彩色图片，在卷积时，每个filter内部也有三个和通道对应的矩阵。三个通道channel一起同步卷积。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://www.jianshu.com/p/ac772ec40192&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/ac772ec40192&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=
      
    
    </summary>
    
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="nlp" scheme="http://yoursite.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch-TensorBoard绘制损失函数曲线</title>
    <link href="http://yoursite.com/2021/02/04/20210204-PyTorch-TensorBoard%E7%BB%98%E5%88%B6%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF/"/>
    <id>http://yoursite.com/2021/02/04/20210204-PyTorch-TensorBoard%E7%BB%98%E5%88%B6%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF/</id>
    <published>2021-02-04T09:52:00.000Z</published>
    <updated>2021-02-04T10:43:48.911Z</updated>
    
    <content type="html"><![CDATA[<p>配置pytorch环境</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade torch torchvision</span><br></pre></td></tr></table></figure><p>安装tensorboard</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboard</span><br></pre></td></tr></table></figure><p>引入相应的Writer类，记录Scalars</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter    </span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(PATH_to_log_dir)</span><br><span class="line"><span class="comment"># Record training loss from each epoch into the writer</span></span><br><span class="line">writer.add_scalar(<span class="string">'Train/Loss'</span>, loss.item(), epoch)</span><br><span class="line">writer.flush()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Record loss and accuracy from the test run into the writer</span></span><br><span class="line">writer.add_scalar(<span class="string">'Test/Loss'</span>, test_loss, epoch)</span><br><span class="line">writer.add_scalar(<span class="string">'Test/Accuracy'</span>, accuracy, epoch)</span><br><span class="line">writer.flush()</span><br></pre></td></tr></table></figure><p>运行TensorBoard</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=/path_to_log_dir/ --port <span class="number">6006</span></span><br></pre></td></tr></table></figure><p>打开浏览器，<a href="http://localhost:6006/" target="_blank" rel="noopener">http://localhost:6006/</a></p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gnbnkfba3qj30gw0fogm9.jpg" style="zoom:50%;" /><p>补充：</p><p>记录图像</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To inspect the input dataset visualize the grid</span></span><br><span class="line">grid = utils.make_grid(images)</span><br><span class="line">writer.add_image(<span class="string">'Dataset/Inspect input grid'</span>, grid, global_step=<span class="number">0</span>)</span><br><span class="line">                 writer.close()</span><br></pre></td></tr></table></figure><p>记录直方图 Histogram</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None)</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;配置pytorch环境&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;
      
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="tensorboard" scheme="http://yoursite.com/tags/tensorboard/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec词向量-CBOW模型</title>
    <link href="http://yoursite.com/2021/01/31/20210131-Word2Vec%E8%AF%8D%E5%90%91%E9%87%8F-CBOW%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2021/01/31/20210131-Word2Vec%E8%AF%8D%E5%90%91%E9%87%8F-CBOW%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-01-31T11:42:00.000Z</published>
    <updated>2021-01-31T12:18:14.175Z</updated>
    
    <content type="html"><![CDATA[<p>转载:[<a href="https://www.zhihu.com/question/44832436/answer/266068967]" target="_blank" rel="noopener">https://www.zhihu.com/question/44832436/answer/266068967]</a></p><h2 id="Word2Vec得到词向量"><a href="#Word2Vec得到词向量" class="headerlink" title="Word2Vec得到词向量"></a>Word2Vec得到词向量</h2><p>文本语料库进行预处理，得到processed corpus，将它们的one-hot向量作为Word2Vec的输入，通过Word2Vec训练低维词向量(word embedding).</p><p>Word2Vec主要有两种训练模型CBOW和Skip-gram. 两种加速算法Negative Sample和Hierarchical Softmax.</p><h2 id="One-hot-Vector-gt-Word2Vec-gt-低维词向量"><a href="#One-hot-Vector-gt-Word2Vec-gt-低维词向量" class="headerlink" title="One-hot Vector -&gt; (Word2Vec) -&gt; 低维词向量"></a>One-hot Vector -&gt; (Word2Vec) -&gt; 低维词向量</h2><p>CBOW: 由中心词周围的词来预测中心词.</p><p>Skip-gram: 由中心词来预测周围的词.</p><h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><img src="https://pic2.zhimg.com/v2-2a319bac1bb7fcae2f4395d2c38674ea_r.jpg?source=1940ef5c" style="zoom:48%;" /><img src="https://pic2.zhimg.com/80/v2-0f439e1bb44c71c8e694cc65cb509263_1440w.jpg?source=1940ef5c" style="zoom:50%;" /><ol><li>输入层：为上下文单词的one-hot vector.「假设单词向量空间维度dim为V，上下文单词数为C」</li><li>所有one-hot向量分别乘以共享的权重矩阵W. 「V*N矩阵，N为自己设定的数，初始化权重矩阵W」</li><li>所得的向量相加求平均作为隐藏层向量，size为1*N</li><li>乘以输出权重矩阵W’ 「W’为N*V维矩阵」</li><li>得到向量「1*V」激活函数处理得到V-dim概率分布，概率最大的index所指示的单词为预测出的中间词(target word)</li><li>与true label的one-hot相比较，误差越小越好</li></ol><p>因此，需要定义loss function(一般为交叉熵代价函数)，采用梯度下降算法更新W和W’。</p><p>训练完毕后，输入层的每个单词与矩阵W(look up table)相乘得到的向量就是所要的词向量(word embedding)。即任何一个单词的one-hot乘以这个矩阵都将得到自己的词向量。</p><p>有了look up table就可以免去训练过程直接查表得到单词的词向量了。</p><h4 id="Example-of-CBOW-Model"><a href="#Example-of-CBOW-Model" class="headerlink" title="Example of CBOW Model"></a>Example of CBOW Model</h4><p><img src="https://pic3.zhimg.com/v2-3e75211b3b675f17a232f29fae0982bc_r.jpg?source=1940ef5c" alt=""></p><p><img src="https://pic1.zhimg.com/v2-abd3c7d6bc76c01266e8ddd32acfe31a_r.jpg?source=1940ef5c" alt=""></p><p><img src="https://pic2.zhimg.com/v2-66655880a87789eaba5dd6f5c5033e94_r.jpg?source=1940ef5c" alt=""></p><p><img src="https://pic1.zhimg.com/v2-5325f4a5d1fbacefd93ccb138b706a69_r.jpg?source=1940ef5c" alt=""></p><p><img src="https://pic4.zhimg.com/v2-1713450fa2a0f37c8cbcce4ffef04baa_r.jpg?source=1940ef5c" alt=""></p><p>假设我们此时得到的概率分布已经达到了设定的迭代次数，那么现在我们训练出来的look up table应该为矩阵W。即，<strong>任何一个单词的one-hot表示乘以这个矩阵都将得到自己的word embedding。</strong></p><h3 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h3><img src="https://pic3.zhimg.com/v2-a54db7c984e6eaf9f06cf21178238fc6_r.jpg?source=1940ef5c" style="zoom:48%;" /> ]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载:[&lt;a href=&quot;https://www.zhihu.com/question/44832436/answer/266068967]&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.zhihu.com/question/448
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="embedding" scheme="http://yoursite.com/tags/embedding/"/>
    
  </entry>
  
  <entry>
    <title>pip更换镜像源</title>
    <link href="http://yoursite.com/2021/01/31/20210131-pip:Anaconda%E6%9B%B4%E6%8D%A2%E9%95%9C%E5%83%8F%E6%BA%90/"/>
    <id>http://yoursite.com/2021/01/31/20210131-pip:Anaconda%E6%9B%B4%E6%8D%A2%E9%95%9C%E5%83%8F%E6%BA%90/</id>
    <published>2021-01-31T00:41:00.000Z</published>
    <updated>2021-01-31T00:49:10.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="国内镜像源"><a href="#国内镜像源" class="headerlink" title="国内镜像源"></a>国内镜像源</h2><p>清华：<a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>阿里云：<a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a><br>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.mirrors.ustc.edu.cn/simple/</a><br>华中理工大学：<a href="http://pypi.hustunique.com/" target="_blank" rel="noopener">http://pypi.hustunique.com/</a><br>山东理工大学：<a href="http://pypi.sdutlinux.org/" target="_blank" rel="noopener">http://pypi.sdutlinux.org/</a><br>豆瓣：<a href="http://pypi.douban.com/simple/" target="_blank" rel="noopener">http://pypi.douban.com/simple/</a></p><h2 id="指定某资源库下载时使用某镜像"><a href="#指定某资源库下载时使用某镜像" class="headerlink" title="指定某资源库下载时使用某镜像"></a>指定某资源库下载时使用某镜像</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将从清华的镜像下载pyspider库</span></span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyspider</span><br></pre></td></tr></table></figure><h2 id="永久修改"><a href="#永久修改" class="headerlink" title="永久修改"></a>永久修改</h2><p>linux/macOS下，修改<code>~/.pip/pip.conf</code>：</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="meta">global</span>]</span><br><span class="line">index-url = https:<span class="comment">//pypi.tuna.tsinghua.edu.cn/simple</span></span><br><span class="line">[<span class="meta">install</span>]</span><br><span class="line">trusted-host=mirrors.aliyun.com</span><br></pre></td></tr></table></figure><p>windows下，直接在user目录中创建一个pip目录，再新建文件pip.ini。（例如：C:\Users\Administrator\pip\pip.ini）内容同上。</p><h2 id="Anaconda修改镜像源"><a href="#Anaconda修改镜像源" class="headerlink" title="Anaconda修改镜像源"></a>Anaconda修改镜像源</h2><p>中科大</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda config --<span class="keyword">add</span> channels https:<span class="comment">//mirrors.ustc.edu.cn/anaconda/pkgs/free/  </span></span><br><span class="line">conda config --<span class="keyword">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><p>清华</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --<span class="keyword">add</span> channels https:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line">conda config --<span class="keyword">add</span> channels https:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span></span><br><span class="line">conda config --<span class="keyword">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><p>或者直接修改<code>~/.condarc</code>文件内容：</p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gn6l8vjt7lj30vo0mymys.jpg" style="zoom:67%;" />]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;国内镜像源&quot;&gt;&lt;a href=&quot;#国内镜像源&quot; class=&quot;headerlink&quot; title=&quot;国内镜像源&quot;&gt;&lt;/a&gt;国内镜像源&lt;/h2&gt;&lt;p&gt;清华：&lt;a href=&quot;https://pypi.tuna.tsinghua.edu.cn/simple&quot; targ
      
    
    </summary>
    
    
      <category term="tips" scheme="http://yoursite.com/categories/tips/"/>
    
    
      <category term="tips" scheme="http://yoursite.com/tags/tips/"/>
    
  </entry>
  
  <entry>
    <title>conda报错-There appear to be 1 leaked semaphore objects to clean up at shutdown</title>
    <link href="http://yoursite.com/2021/01/30/20210131-conda%E6%8A%A5%E9%94%99-There-appear-to-be-1-leaked-semaphore-objects-to-clean-up-at-shutdown/"/>
    <id>http://yoursite.com/2021/01/30/20210131-conda%E6%8A%A5%E9%94%99-There-appear-to-be-1-leaked-semaphore-objects-to-clean-up-at-shutdown/</id>
    <published>2021-01-30T15:58:00.000Z</published>
    <updated>2021-01-31T00:52:42.705Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>安装Anaconda后运行<code>conda env list</code>时报警告⚠️如下：</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gn673z4kxej30vo08wta2.jpg" alt=""></p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>使用<code>conda update --all</code>命令</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h2&gt;&lt;p&gt;安装Anaconda后运行&lt;code&gt;conda env list&lt;/code&gt;时报警告⚠️如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;htt
      
    
    </summary>
    
    
      <category term="tips" scheme="http://yoursite.com/categories/tips/"/>
    
    
      <category term="tips" scheme="http://yoursite.com/tags/tips/"/>
    
  </entry>
  
  <entry>
    <title>Error:Could not install packages due to an EnvironmentError-Errno 28 No space left on device</title>
    <link href="http://yoursite.com/2021/01/28/20210128-ErrorCould-not-install-packages-due-to-an-EnvironmentError-Errno-28-No-space-left-on-device/"/>
    <id>http://yoursite.com/2021/01/28/20210128-ErrorCould-not-install-packages-due-to-an-EnvironmentError-Errno-28-No-space-left-on-device/</id>
    <published>2021-01-28T08:11:00.000Z</published>
    <updated>2021-01-28T09:04:39.870Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Error-Info"><a href="#Error-Info" class="headerlink" title="Error Info"></a>Error Info</h2><p>pip install的时候报错：</p><p><strong>Error</strong>: Could not install packages due to an EnvironmentError: [Errno 28] No space left on device</p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>查看磁盘空间</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gn3hbhaalfj30ui094myo.jpg" alt=""></p><p>看到挂载点/devdata还有1.4T空间，不是磁盘空间不够。</p><p>可能由于缓存空间不够了，即设备分配给/tmp的磁盘空间不够了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(lisaipeng) xddz@xddz:&#x2F;devdata$ cd &#x2F;tmp</span><br><span class="line">(lisaipeng) xddz@xddz:&#x2F;tmp$ ls</span><br><span class="line">(lisaipeng) xddz@xddz:&#x2F;tmp$ rm -rf *</span><br></pre></td></tr></table></figure><p>清空/tmp空间后(挂载点/空间清理出174M)，pip install仍报错设备空间不够。</p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gn3hlz4ejsj30mq08ut9u.jpg" style="zoom:67%;" /><p>猜测，尽管将pip install安装包路径指定到/devdata中，但是仍需要/挂载点的空间用作安装包缓存。因此仍然需要清理/挂载点空间。</p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>使用如下命令，查看设备空间使用情况(各一级文件夹占用)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) xddz@xddz:~$ du -h --max-depth&#x3D;1</span><br></pre></td></tr></table></figure><p>将大文件夹移动到/devdata挂载点。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) xddz@xddz:/devdata$ mv -r /home/xddz/Downloads /devdata/Downloads</span><br></pre></td></tr></table></figure><p>清理出9G空间。</p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gn3hsdp621j30n009275k.jpg" style="zoom:67%;" /><p>再次pip install, 可以正常安装包了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Error-Info&quot;&gt;&lt;a href=&quot;#Error-Info&quot; class=&quot;headerlink&quot; title=&quot;Error Info&quot;&gt;&lt;/a&gt;Error Info&lt;/h2&gt;&lt;p&gt;pip install的时候报错：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Error
      
    
    </summary>
    
    
      <category term="tip" scheme="http://yoursite.com/categories/tip/"/>
    
    
      <category term="tip" scheme="http://yoursite.com/tags/tip/"/>
    
  </entry>
  
  <entry>
    <title>服务器conda创建指定路径虚拟环境时，不显示名称无法找到</title>
    <link href="http://yoursite.com/2021/01/28/20210128-%E6%9C%8D%E5%8A%A1%E5%99%A8conda%E5%88%9B%E5%BB%BA%E6%8C%87%E5%AE%9A%E8%B7%AF%E5%BE%84%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%97%B6%EF%BC%8C%E4%B8%8D%E6%98%BE%E7%A4%BA%E5%90%8D%E7%A7%B0%E6%97%A0%E6%B3%95%E6%89%BE%E5%88%B0/"/>
    <id>http://yoursite.com/2021/01/28/20210128-%E6%9C%8D%E5%8A%A1%E5%99%A8conda%E5%88%9B%E5%BB%BA%E6%8C%87%E5%AE%9A%E8%B7%AF%E5%BE%84%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E6%97%B6%EF%BC%8C%E4%B8%8D%E6%98%BE%E7%A4%BA%E5%90%8D%E7%A7%B0%E6%97%A0%E6%B3%95%E6%89%BE%E5%88%B0/</id>
    <published>2021-01-28T06:47:00.000Z</published>
    <updated>2021-01-28T08:10:44.680Z</updated>
    
    <content type="html"><![CDATA[<h2 id="指定路径conda虚拟环境"><a href="#指定路径conda虚拟环境" class="headerlink" title="指定路径conda虚拟环境"></a>指定路径conda虚拟环境</h2><p>创建</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --prefix=/root/python36/venv-name python=<span class="number">3.6</span><span class="number">.8</span></span><br></pre></td></tr></table></figure><p>删除</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove -p /root/python36/venv-name --all</span><br></pre></td></tr></table></figure><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>创建的conda环境不显示名称，且找不到也无法激活(activate).</p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gn3eu5m15fj30nm06mdgl.jpg" style="zoom:67%;" /><p><code>conda activate lisaipeng</code>会提示无法找到该环境。</p><h2 id="解决途径"><a href="#解决途径" class="headerlink" title="解决途径"></a>解决途径</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda config --add pkgs_dirs /devdata/saipeng.li/pkgs</span><br><span class="line">conda config --add envs_dirs /devdata/saipeng.li/envs</span><br></pre></td></tr></table></figure><p>查看conda info</p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gn3f1ceo3aj30j404ijrt.jpg" style="zoom:67%;" /><p>查看conda env list</p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gn3f24fxawj30no05ogm1.jpg" style="zoom:67%;" /><p>get it.</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>所创建的conda虚拟环境的pip install会将包安装在全局pip路径下，应该在虚拟环境中安装pip，将其安装到自己的虚拟环境目录中.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pip</span><br></pre></td></tr></table></figure><p>~/.bashrc中添加：</p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gn3gk43rblj30mw0140ta.jpg" alt="image-20210128154953128" style="zoom:50%;" /><p>(lisaipeng) xddz@xddz:/devdata/saipeng.li/envs/lisaipeng/lib/python3.6$ vi site.py</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gn3gw3unh0j30v001ywg5.jpg" alt=""></p><p>查看python site-info.</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gn3gxx73hhj310209udn1.jpg" alt="image-20210128160309113"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;指定路径conda虚拟环境&quot;&gt;&lt;a href=&quot;#指定路径conda虚拟环境&quot; class=&quot;headerlink&quot; title=&quot;指定路径conda虚拟环境&quot;&gt;&lt;/a&gt;指定路径conda虚拟环境&lt;/h2&gt;&lt;p&gt;创建&lt;/p&gt;
&lt;figure class=&quot;high
      
    
    </summary>
    
    
      <category term="tips" scheme="http://yoursite.com/categories/tips/"/>
    
    
      <category term="tips" scheme="http://yoursite.com/tags/tips/"/>
    
  </entry>
  
  <entry>
    <title>SPC(Single-pass Clsutering)</title>
    <link href="http://yoursite.com/2021/01/26/20210126-SPC(Single-pass-Clsutering)/"/>
    <id>http://yoursite.com/2021/01/26/20210126-SPC(Single-pass-Clsutering)/</id>
    <published>2021-01-26T06:50:00.000Z</published>
    <updated>2021-01-27T08:38:13.505Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Single-pass-Clustering"><a href="#Single-pass-Clustering" class="headerlink" title="Single-pass Clustering"></a>Single-pass Clustering</h2><p>在文本主题聚类中，single-pass clustering(单遍聚类)算法通常比K-means更为有效。每个文档只需要流过算法一次，其不需要指定类目数量，可以通过设定相似度阈值来限定聚类数量。</p><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p>Single-pass算法顺序地处理文本</p><ol><li><strong>以第一篇文档为种子，建立一个新主题</strong>。</li><li>计算新进入文档与已有主题的<strong>相似度</strong>：<ul><li>如果与已有主题相似度大于阈值，将该文档加入到与它<strong>相似度最大</strong>的且<strong>大于一定阈值</strong>的主题中；</li><li>如果与所有已有主题相似度都小于阈值，则以该文档为聚类种子，建立新的主题类别；</li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Single-pass-Clustering&quot;&gt;&lt;a href=&quot;#Single-pass-Clustering&quot; class=&quot;headerlink&quot; title=&quot;Single-pass Clustering&quot;&gt;&lt;/a&gt;Single-pass Clusteri
      
    
    </summary>
    
    
      <category term="Cluster" scheme="http://yoursite.com/categories/Cluster/"/>
    
    
      <category term="Cluster" scheme="http://yoursite.com/tags/Cluster/"/>
    
  </entry>
  
  <entry>
    <title>序列标注任务的准确率和召回率</title>
    <link href="http://yoursite.com/2021/01/26/20210126-%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%87%86%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/"/>
    <id>http://yoursite.com/2021/01/26/20210126-%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%87%86%E7%A1%AE%E7%8E%87%E5%92%8C%E5%8F%AC%E5%9B%9E%E7%8E%87/</id>
    <published>2021-01-26T01:58:00.000Z</published>
    <updated>2021-01-26T02:53:37.930Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>引：<a href="https://segmentfault.com/a/1190000021975136" target="_blank" rel="noopener">https://segmentfault.com/a/1190000021975136</a></p></blockquote><h2 id="序列标注的准确率和召回率"><a href="#序列标注的准确率和召回率" class="headerlink" title="序列标注的准确率和召回率"></a>序列标注的准确率和召回率</h2><h3 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h3><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><ul><li>准确率: accuracy = 预测对的元素个数/总的元素个数</li><li>查准率：precision = 预测正确的实体个数 / 预测的实体总个数</li><li>召回率：recall = 预测正确的实体个数 / 标注的实体总个数</li><li>F1值： F1 = 2*准确率*召回率 / (准确率 + 召回率)</li></ul><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>E.g</p><p>  在序列标注算法中，一般我们会形成如下的序列列表，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'O'</span>, <span class="string">'O'</span>, <span class="string">'B-MISC'</span>, <span class="string">'I-MISC'</span>, <span class="string">'B-MISC'</span>, <span class="string">'I-MISC'</span>, <span class="string">'O'</span>, <span class="string">'B-PER'</span>, <span class="string">'I-PER'</span>]</span><br></pre></td></tr></table></figure><p>序列标注算法的格式有BIO，IOBES，BMES等.</p><ul><li>B，即Begin，表示开始</li><li>I，即Intermediate，表示中间</li><li>E，即End，表示结尾</li><li>S，即Single，表示单个字符</li><li>O，即Other，表示其他，用于标记无关字符</li></ul><h4 id="·实体·和·标签·"><a href="#·实体·和·标签·" class="headerlink" title="·实体·和·标签·"></a>·实体·和·标签·</h4><p><strong>实体</strong>指的是从B开头标签开始的，同一类型的，非O的连续标签序列。如[‘B-MISC’, ‘I-MISC’, ‘I-MISC’].</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_true = </span><br><span class="line">[<span class="string">'O'</span>, <span class="string">'O'</span>, <span class="string">'O'</span>, <span class="string">'B-MISC'</span>, <span class="string">'I-MISC'</span>, <span class="string">'I-MISC'</span>, <span class="string">'O'</span>, <span class="string">'B-PER'</span>, <span class="string">'I-PER'</span>]</span><br><span class="line"></span><br><span class="line">y_pred = </span><br><span class="line">[<span class="string">'O'</span>, <span class="string">'O'</span>, <span class="string">'B-MISC'</span>, <span class="string">'I-MISC'</span>, <span class="string">'B-MISC'</span>, <span class="string">'I-MISC'</span>, <span class="string">'O'</span>, <span class="string">'B-PER'</span>, <span class="string">'I-PER'</span>]</span><br></pre></td></tr></table></figure><p>列表中一共有9个元素：’O’, ‘O’, ‘O’, ‘B-MISC’, ‘I-MISC’, ‘I-MISC’, ‘O’, ‘B-PER’, ‘I-PER’</p><p>预测对的元素个数有6个：’O’(yes), ‘O’(yes), ‘O’, ‘B-MISC’, ‘I-MISC’, ‘I-MISC’(yes), ‘O’(yes), ‘B-PER’(yes), ‘I-PER’(yes)</p><p>标注的实体总个数为2个: [‘B-MISC’, ‘I-MISC’, ‘I-MISC’], [‘B-PER’, ‘I-PER’]</p><p>预测的实体总个数为3个：[‘B-MISC’, ‘I-MISC’],[‘B-MISC’, ‘I-MISC’],[‘B-PER’, ‘I-PER’]</p><p>预测正确的实体个数为1个：[‘B-PER’, ‘I-PER’]</p><p>因此：precision=1/3, recall=1/2, f1=0.4</p><h4 id="seqeval"><a href="#seqeval" class="headerlink" title="seqeval"></a>seqeval</h4><p>一般的序列标注算法，是用<code>conlleval.pl</code>脚本实现，但这是用perl语言实现的。在Python中，也有相应的序列标注算法的模型效果评估的第三方模块，那就是<code>seqeval</code>，其官网网址为：<a href="https://pypi.org/project/seqeval/0.0.3/" target="_blank" rel="noopener">https://pypi.org/project/seqeval/0.0.3/</a> 。</p><p><code>seqeval</code>支持<code>BIO</code>，<code>IOBES</code>标注模式，可用于命名实体识别，词性标注，语义角色标注等任务的评估。</p><p>E.g</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> seqeval.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">from</span> seqeval.metrics <span class="keyword">import</span> precision_score</span><br><span class="line"><span class="keyword">from</span> seqeval.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> seqeval.metrics <span class="keyword">import</span> recall_score</span><br><span class="line"><span class="keyword">from</span> seqeval.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">y_true = [<span class="string">'O'</span>, <span class="string">'O'</span>, <span class="string">'O'</span>, <span class="string">'B-MISC'</span>, <span class="string">'I-MISC'</span>, <span class="string">'I-MISC'</span>, <span class="string">'O'</span>, <span class="string">'B-PER'</span>, <span class="string">'I-PER'</span>]</span><br><span class="line">y_pred = [<span class="string">'O'</span>, <span class="string">'O'</span>, <span class="string">'B-MISC'</span>, <span class="string">'I-MISC'</span>, <span class="string">'B-MISC'</span>, <span class="string">'I-MISC'</span>, <span class="string">'O'</span>, <span class="string">'B-PER'</span>, <span class="string">'I-PER'</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"accuary: "</span>, accuracy_score(y_true, y_pred))</span><br><span class="line">print(<span class="string">"p: "</span>, precision_score(y_true, y_pred))</span><br><span class="line">print(<span class="string">"r: "</span>, recall_score(y_true, y_pred))</span><br><span class="line">print(<span class="string">"f1: "</span>, f1_score(y_true, y_pred))</span><br><span class="line">print(<span class="string">"classification report: "</span>)</span><br><span class="line">print(classification_report(y_true, y_pred))</span><br></pre></td></tr></table></figure><p>Output</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">accuary:  <span class="number">0.6666666666666666</span></span><br><span class="line">p:  <span class="number">0.3333333333333333</span></span><br><span class="line">r:  <span class="number">0.5</span></span><br><span class="line">f1:  <span class="number">0.4</span></span><br><span class="line">classification report: </span><br><span class="line">           precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">     MISC       <span class="number">0.00</span>      <span class="number">0.00</span>      <span class="number">0.00</span>         <span class="number">1</span></span><br><span class="line">      PER       <span class="number">1.00</span>      <span class="number">1.00</span>      <span class="number">1.00</span>         <span class="number">1</span></span><br><span class="line"></span><br><span class="line">micro avg       <span class="number">0.33</span>      <span class="number">0.50</span>      <span class="number">0.40</span>         <span class="number">2</span></span><br><span class="line">macro avg       <span class="number">0.50</span>      <span class="number">0.50</span>      <span class="number">0.50</span>         <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> seqeval.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> seqeval.metrics <span class="keyword">import</span> precision_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> seqeval.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> seqeval.metrics <span class="keyword">import</span> recall_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> seqeval.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_true = [[<span class="string">'B'</span>,<span class="string">'I'</span>,<span class="string">'B'</span>,<span class="string">'I'</span>,<span class="string">'B'</span>,<span class="string">'I'</span>,<span class="string">'I'</span>,<span class="string">'I'</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_pred = [[<span class="string">'B'</span>,<span class="string">'I'</span>,<span class="string">'I'</span>,<span class="string">'I'</span>,<span class="string">'B'</span>,<span class="string">'I'</span>,<span class="string">'I'</span>,<span class="string">'I'</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"accuary: "</span>, accuracy_score(y_true, y_pred))</span><br><span class="line">accuary:  <span class="number">0.875</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"p: "</span>, precision_score(y_true, y_pred))</span><br><span class="line">p:  <span class="number">0.5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"r: "</span>, recall_score(y_true, y_pred))</span><br><span class="line">r:  <span class="number">0.3333333333333333</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"f1: "</span>, f1_score(y_true, y_pred))</span><br><span class="line">f1:  <span class="number">0.4</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"classification report: "</span>)</span><br><span class="line">classification report: </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(classification_report(y_true, y_pred))</span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           _       <span class="number">0.50</span>      <span class="number">0.33</span>      <span class="number">0.40</span>         <span class="number">3</span></span><br><span class="line"></span><br><span class="line">   micro avg       <span class="number">0.50</span>      <span class="number">0.33</span>      <span class="number">0.40</span>         <span class="number">3</span></span><br><span class="line">   macro avg       <span class="number">0.50</span>      <span class="number">0.33</span>      <span class="number">0.40</span>         <span class="number">3</span></span><br><span class="line">weighted avg       <span class="number">0.50</span>      <span class="number">0.33</span>      <span class="number">0.40</span>         <span class="number">3</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;引：&lt;a href=&quot;https://segmentfault.com/a/1190000021975136&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://segmentfault.com/a/1190000021
      
    
    </summary>
    
    
      <category term="NER" scheme="http://yoursite.com/categories/NER/"/>
    
    
      <category term="NER" scheme="http://yoursite.com/tags/NER/"/>
    
  </entry>
  
  <entry>
    <title>Hexo的Next主题翻页箭头显示为&lt;i class=&quot;fa fa-angle-right&quot;&gt;&lt;/i&gt;问题</title>
    <link href="http://yoursite.com/2021/01/24/20210124-Hexo%E7%9A%84Next%E4%B8%BB%E9%A2%98%E7%BF%BB%E9%A1%B5%E7%AE%AD%E5%A4%B4%E6%98%BE%E7%A4%BA%E4%B8%BAi-class=fa-fa-angle-righti%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2021/01/24/20210124-Hexo%E7%9A%84Next%E4%B8%BB%E9%A2%98%E7%BF%BB%E9%A1%B5%E7%AE%AD%E5%A4%B4%E6%98%BE%E7%A4%BA%E4%B8%BAi-class=fa-fa-angle-righti%E9%97%AE%E9%A2%98/</id>
    <published>2021-01-24T11:49:00.000Z</published>
    <updated>2021-01-24T12:00:10.590Z</updated>
    
    <content type="html"><![CDATA[<h2 id="出现问题"><a href="#出现问题" class="headerlink" title="出现问题"></a>出现问题</h2><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gmz15x1b6hj30cd01ka9v.jpg" alt=""></p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>修改代码：<code>themes\next\layout\_partials\pagination.swig</code></p><p>旧：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if page.prev or page.next %&#125;</span><br><span class="line">  <span class="tag">&lt;<span class="name">nav</span> <span class="attr">class</span>=<span class="string">"pagination"</span>&gt;</span></span><br><span class="line">    &#123;&#123;</span><br><span class="line">      paginator(&#123;</span><br><span class="line">        prev_text: '<span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fa fa-angle-left"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span>',</span><br><span class="line">        next_text: '<span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fa fa-angle-right"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span>',</span><br><span class="line">        mid_size: 1</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;&#125;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">nav</span>&gt;</span></span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>新：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if page.prev or page.next %&#125;</span><br><span class="line">  &lt;nav class&#x3D;&quot;pagination&quot;&gt;</span><br><span class="line">    &#123;&#123;</span><br><span class="line">      paginator(&#123;</span><br><span class="line">        prev_text: &#39;&lt;&#39;,</span><br><span class="line">        next_text: &#39;&gt;&#39;,</span><br><span class="line">        mid_size: 1</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;&#125;</span><br><span class="line">  &lt;&#x2F;nav&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;出现问题&quot;&gt;&lt;a href=&quot;#出现问题&quot; class=&quot;headerlink&quot; title=&quot;出现问题&quot;&gt;&lt;/a&gt;出现问题&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/008eGmZEly1gmz15x1b6hj
      
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/categories/hexo/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>概念辨析-Epoch,Batch,Iteration</title>
    <link href="http://yoursite.com/2021/01/24/20210124-%E6%A6%82%E5%BF%B5%E8%BE%A8%E6%9E%90-Epoch,Batch,Iteration/"/>
    <id>http://yoursite.com/2021/01/24/20210124-%E6%A6%82%E5%BF%B5%E8%BE%A8%E6%9E%90-Epoch,Batch,Iteration/</id>
    <published>2021-01-23T16:00:00.000Z</published>
    <updated>2021-01-25T02:46:35.463Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Epoch-Batch-Iteration"><a href="#Epoch-Batch-Iteration" class="headerlink" title="Epoch, Batch, Iteration"></a>Epoch, Batch, Iteration</h2><p><code>Epoch</code>: 将所有训练样本训练一次的过程，即一个完整的数据集(所有训练样本)在神经网络中都进行了一次正向传播和一次反向传播。</p><p><code>Batch</code>: 当一个Epoch的样本数量过于庞大，则需要将其分为多个小块(Batch)进行训练。一个epoch需要经历多个Iteration(每次Iteration为一个Batch的Iteration)来完成。</p><p><code>Iteration</code>: 训练一个Batch就是一次Iteration。</p><h2 id="梯度下降方式"><a href="#梯度下降方式" class="headerlink" title="梯度下降方式"></a>梯度下降方式</h2><p>(In one Epoch)</p><table><thead><tr><th>梯度下降方式</th><th>Training Set Size</th><th>Batch Size</th><th>Number of Batches</th></tr></thead><tbody><tr><td><code>BGD</code>批量梯度下降法</td><td>N</td><td>N</td><td>1</td></tr><tr><td><code>SGD</code>随机梯度下降法</td><td>N</td><td>1</td><td>N</td></tr><tr><td><code>Mini-Batch</code>小批量梯度下降法</td><td>N</td><td>B</td><td>$\frac{N}{B}$+1</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Epoch-Batch-Iteration&quot;&gt;&lt;a href=&quot;#Epoch-Batch-Iteration&quot; class=&quot;headerlink&quot; title=&quot;Epoch, Batch, Iteration&quot;&gt;&lt;/a&gt;Epoch, Batch, Iterati
      
    
    </summary>
    
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="nlp" scheme="http://yoursite.com/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>My first blog</title>
    <link href="http://yoursite.com/2020/01/14/2020-01-14-My-first-blog/"/>
    <id>http://yoursite.com/2020/01/14/2020-01-14-My-first-blog/</id>
    <published>2020-01-14T14:32:24.000Z</published>
    <updated>2021-01-24T11:52:14.414Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"Hello world"</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Hello-World&quot;&gt;&lt;a href=&quot;#Hello-World&quot; class=&quot;headerlink&quot; title=&quot;Hello World&quot;&gt;&lt;/a&gt;Hello World&lt;/h2&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;
      
    
    </summary>
    
    
      <category term="杂" scheme="http://yoursite.com/categories/%E6%9D%82/"/>
    
    
      <category term="杂" scheme="http://yoursite.com/tags/%E6%9D%82/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode-0 模版</title>
    <link href="http://yoursite.com/2019/11/30/2020-00-00-LeetCode-0-%E6%A8%A1%E7%89%88/"/>
    <id>http://yoursite.com/2019/11/30/2020-00-00-LeetCode-0-%E6%A8%A1%E7%89%88/</id>
    <published>2019-11-29T16:00:00.000Z</published>
    <updated>2020-07-21T11:10:04.575Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;
      
    
    </summary>
    
    
      <category term="leetcode" scheme="http://yoursite.com/categories/leetcode/"/>
    
    
      <category term="leetcode" scheme="http://yoursite.com/tags/leetcode/"/>
    
      <category term="Medium" scheme="http://yoursite.com/tags/Medium/"/>
    
  </entry>
  
</feed>
